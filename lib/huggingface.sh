#!/usr/bin/env bash
# huggingface.sh - HuggingFace Datasets ä¸‹è¼‰å·¥å…·
# æ³¨æ„ï¼šé æœŸè¢«å…¶ä»– script ç”¨ `source` è¼‰å…¥
# ä¸åœ¨é€™è£¡ set -euo pipefailï¼Œäº¤çµ¦å‘¼å«ç«¯æ±ºå®šã€‚

if [[ -n "${HUGGINGFACE_SH_LOADED:-}" ]]; then
  return 0 2>/dev/null || exit 0
fi
HUGGINGFACE_SH_LOADED=1

_hf_lib_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
. "${_hf_lib_dir}/core.sh"

########################################
# hf_download DATASET_ID OUTPUT_DIR [SPLIT]
#
# åŠŸèƒ½ï¼š
#   - å¾ž HuggingFace Datasets ä¸‹è¼‰è³‡æ–™é›†
#
# åƒæ•¸ï¼š
#   DATASET_ID: HuggingFace è³‡æ–™é›† IDï¼ˆå¦‚ "bigdata-pw/aops"ï¼‰
#   OUTPUT_DIR: è¼¸å‡ºç›®éŒ„
#   SPLIT: å¯é¸ï¼ŒæŒ‡å®šè¦ä¸‹è¼‰çš„ splitï¼ˆtrain/test/validationï¼‰
#
# å›žå‚³å€¼ï¼š
#   0  = æˆåŠŸ
#   >0 = å¤±æ•—
#
# ä¾è³´ï¼š
#   curl, jq
#
# èªªæ˜Žï¼š
#   HuggingFace Datasets æœ‰å¤šç¨®å­˜å–æ–¹å¼ï¼š
#   1. Datasets Hub APIï¼ˆéœ€è¦ HF_TOKENï¼‰
#   2. ç›´æŽ¥ä¸‹è¼‰ Parquet/JSON æª”æ¡ˆ
#   æœ¬å‡½å¼å˜—è©¦æ–¹æ³• 2ï¼Œä»¥é¿å…èªè­‰éœ€æ±‚
########################################
hf_download() {
  local dataset_id="$1"
  local output_dir="$2"
  local split="${3:-train}"

  require_cmd curl || return 1
  require_cmd jq || return 1

  if [[ -z "$dataset_id" || -z "$output_dir" ]]; then
    echo "âŒ [hf_download] ç”¨æ³•ï¼šhf_download DATASET_ID OUTPUT_DIR [SPLIT]" >&2
    return 1
  fi

  mkdir -p "$output_dir"

  local base_url="https://huggingface.co/datasets/${dataset_id}"
  local api_url="https://huggingface.co/api/datasets/${dataset_id}"

  echo "ðŸ“¦ [hf_download] ä¸‹è¼‰è³‡æ–™é›†ï¼š$dataset_id" >&2

  # å˜—è©¦å–å¾—è³‡æ–™é›†è³‡è¨Š
  local dataset_info
  dataset_info="$(curl -sf "$api_url" 2>/dev/null)" || {
    echo "âš ï¸  [hf_download] ç„¡æ³•å–å¾—è³‡æ–™é›†è³‡è¨Šï¼Œå˜—è©¦ç›´æŽ¥ä¸‹è¼‰..." >&2
  }

  # å˜—è©¦ä¸‹è¼‰ Parquet æª”æ¡ˆï¼ˆHuggingFace é è¨­æ ¼å¼ï¼‰
  local parquet_url="${base_url}/resolve/main/data/${split}-00000-of-00001.parquet"
  local parquet_file="${output_dir}/${split}.parquet"

  if curl -sf -L -o "$parquet_file" "$parquet_url" 2>/dev/null; then
    echo "âœ… [hf_download] Parquet æª”æ¡ˆä¸‹è¼‰æˆåŠŸï¼š$parquet_file" >&2
    return 0
  fi

  # å˜—è©¦ä¸‹è¼‰ JSONL æª”æ¡ˆ
  local jsonl_url="${base_url}/resolve/main/data/${split}.jsonl"
  local jsonl_file="${output_dir}/${split}.jsonl"

  if curl -sf -L -o "$jsonl_file" "$jsonl_url" 2>/dev/null; then
    echo "âœ… [hf_download] JSONL æª”æ¡ˆä¸‹è¼‰æˆåŠŸï¼š$jsonl_file" >&2
    return 0
  fi

  # å˜—è©¦ä¸‹è¼‰ JSON æª”æ¡ˆ
  local json_url="${base_url}/resolve/main/data/${split}.json"
  local json_file="${output_dir}/${split}.json"

  if curl -sf -L -o "$json_file" "$json_url" 2>/dev/null; then
    echo "âœ… [hf_download] JSON æª”æ¡ˆä¸‹è¼‰æˆåŠŸï¼š$json_file" >&2
    # è½‰æ›ç‚º JSONL
    if [[ -f "$json_file" ]]; then
      jq -c '.[]' "$json_file" > "${output_dir}/${split}.jsonl" 2>/dev/null || true
    fi
    return 0
  fi

  echo "âŒ [hf_download] ç„¡æ³•ä¸‹è¼‰è³‡æ–™é›†ï¼š$dataset_id" >&2
  echo "   è«‹ç¢ºèªè³‡æ–™é›† ID æ­£ç¢ºä¸”ç‚ºå…¬é–‹è³‡æ–™é›†" >&2
  return 1
}

########################################
# hf_download_file DATASET_ID FILE_PATH OUTPUT_FILE
#
# åŠŸèƒ½ï¼š
#   - å¾ž HuggingFace ä¸‹è¼‰ç‰¹å®šæª”æ¡ˆ
#
# åƒæ•¸ï¼š
#   DATASET_ID: HuggingFace è³‡æ–™é›† ID
#   FILE_PATH: è³‡æ–™é›†å…§çš„æª”æ¡ˆè·¯å¾‘
#   OUTPUT_FILE: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
#
# å›žå‚³å€¼ï¼š
#   0  = æˆåŠŸ
#   >0 = å¤±æ•—
########################################
hf_download_file() {
  local dataset_id="$1"
  local file_path="$2"
  local output_file="$3"

  require_cmd curl || return 1

  if [[ -z "$dataset_id" || -z "$file_path" || -z "$output_file" ]]; then
    echo "âŒ [hf_download_file] ç”¨æ³•ï¼šhf_download_file DATASET_ID FILE_PATH OUTPUT_FILE" >&2
    return 1
  fi

  local url="https://huggingface.co/datasets/${dataset_id}/resolve/main/${file_path}"

  local max_retries=3
  local retry_delay=2

  for ((attempt=1; attempt<=max_retries; attempt++)); do
    if curl -sf -L -o "$output_file" "$url" 2>/dev/null; then
      echo "âœ… [hf_download_file] ä¸‹è¼‰æˆåŠŸï¼š$output_file" >&2
      return 0
    fi

    if [[ $attempt -lt $max_retries ]]; then
      echo "âš ï¸  [hf_download_file] ä¸‹è¼‰å¤±æ•—ï¼Œé‡è©¦ $attempt/$max_retries..." >&2
      sleep $retry_delay
    fi
  done

  echo "âŒ [hf_download_file] ç„¡æ³•ä¸‹è¼‰ï¼š$url" >&2
  return 1
}

########################################
# hf_list_files DATASET_ID
#
# åŠŸèƒ½ï¼š
#   - åˆ—å‡º HuggingFace è³‡æ–™é›†ä¸­çš„æª”æ¡ˆ
#
# åƒæ•¸ï¼š
#   DATASET_ID: HuggingFace è³‡æ–™é›† ID
#
# stdout:
#   æª”æ¡ˆåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰
########################################
hf_list_files() {
  local dataset_id="$1"

  require_cmd curl || return 1
  require_cmd jq || return 1

  if [[ -z "$dataset_id" ]]; then
    echo "âŒ [hf_list_files] ç”¨æ³•ï¼šhf_list_files DATASET_ID" >&2
    return 1
  fi

  local api_url="https://huggingface.co/api/datasets/${dataset_id}/tree/main"

  curl -sf "$api_url" 2>/dev/null | jq -r '.[].path' 2>/dev/null || {
    echo "âŒ [hf_list_files] ç„¡æ³•å–å¾—æª”æ¡ˆåˆ—è¡¨ï¼š$dataset_id" >&2
    return 1
  }
}

########################################
# hf_dataset_info DATASET_ID
#
# åŠŸèƒ½ï¼š
#   - å–å¾— HuggingFace è³‡æ–™é›†çš„åŸºæœ¬è³‡è¨Š
#
# åƒæ•¸ï¼š
#   DATASET_ID: HuggingFace è³‡æ–™é›† ID
#
# stdout:
#   JSON æ ¼å¼çš„è³‡æ–™é›†è³‡è¨Š
########################################
hf_dataset_info() {
  local dataset_id="$1"

  require_cmd curl || return 1

  if [[ -z "$dataset_id" ]]; then
    echo "âŒ [hf_dataset_info] ç”¨æ³•ï¼šhf_dataset_info DATASET_ID" >&2
    return 1
  fi

  local api_url="https://huggingface.co/api/datasets/${dataset_id}"

  curl -sf "$api_url" 2>/dev/null || {
    echo "âŒ [hf_dataset_info] ç„¡æ³•å–å¾—è³‡æ–™é›†è³‡è¨Šï¼š$dataset_id" >&2
    return 1
  }
}

########################################
# hf_parquet_to_jsonl PARQUET_FILE JSONL_FILE
#
# åŠŸèƒ½ï¼š
#   - å°‡ Parquet æª”æ¡ˆè½‰æ›ç‚º JSONL
#
# åƒæ•¸ï¼š
#   PARQUET_FILE: è¼¸å…¥ Parquet æª”æ¡ˆ
#   JSONL_FILE: è¼¸å‡º JSONL æª”æ¡ˆ
#
# ä¾è³´ï¼š
#   Python 3 + pyarrow æˆ– pandas
#
# å›žå‚³å€¼ï¼š
#   0  = æˆåŠŸ
#   >0 = å¤±æ•—
########################################
hf_parquet_to_jsonl() {
  local parquet_file="$1"
  local jsonl_file="$2"

  if [[ -z "$parquet_file" || -z "$jsonl_file" ]]; then
    echo "âŒ [hf_parquet_to_jsonl] ç”¨æ³•ï¼šhf_parquet_to_jsonl PARQUET_FILE JSONL_FILE" >&2
    return 1
  fi

  if [[ ! -f "$parquet_file" ]]; then
    echo "âŒ [hf_parquet_to_jsonl] æª”æ¡ˆä¸å­˜åœ¨ï¼š$parquet_file" >&2
    return 1
  fi

  # å˜—è©¦ä½¿ç”¨ Python è½‰æ›
  if command -v python3 >/dev/null 2>&1; then
    python3 - "$parquet_file" "$jsonl_file" << 'PYTHON_SCRIPT' 2>/dev/null && return 0
import sys
import json

parquet_file = sys.argv[1]
jsonl_file = sys.argv[2]

try:
    import pyarrow.parquet as pq
    table = pq.read_table(parquet_file)
    df = table.to_pandas()
except ImportError:
    try:
        import pandas as pd
        df = pd.read_parquet(parquet_file)
    except ImportError:
        print("âŒ éœ€è¦å®‰è£ pyarrow æˆ– pandas", file=sys.stderr)
        sys.exit(1)

with open(jsonl_file, 'w', encoding='utf-8') as f:
    for _, row in df.iterrows():
        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\n')

print(f"âœ… è½‰æ›æˆåŠŸï¼š{jsonl_file}", file=sys.stderr)
PYTHON_SCRIPT
  fi

  echo "âŒ [hf_parquet_to_jsonl] éœ€è¦ Python 3 + pyarrow æˆ– pandas" >&2
  return 1
}
